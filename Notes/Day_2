# -*- coding: utf-8 -*-
"""Day_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PleX1x84qd0GlTBtaHXAT59w-F17HxVb

# Supervised vs Unsupervised Learning

* Supervised Learning
* * is done using features for example, when predicting stock prices, the machine learning engineer is given Prices, and a time, to learn from. ( Used for classicfiation and Regression )
* Unsupervised Learning
* * is done using data that has no labels and only features.( Used To Find Patterns in data ) (ex: Market Segmentation )

# KNN vs K-Means Clustering
* KNN( K Nearest Neighbor ) (Supervised Model)
* * This alogirthm is used to map closest values together in a starta or group based on distance. Used for classification and regression
* K-Mean Clustering ( Unsupervised model )
* * Is Similar but the algorthim chooses K centroids for each group, then find the mean of that group and iterativily runs this process to reduce variance.

# How to ensure model is not overfitting
* Consisntecy between the training, test, and validation sets can express insites into how the model is performing.
* Cross Validation
* K-Fold Cross Validation
* Grid Search
* Regularization
* Hyperparameter Tuning
* Model Retraining

# THINGS I NEED TO REVIEW
Explain Ensemble learning.
In ensemble learning, many base models like classifiers and regressors are generated and combined together so that they give better results. It is used when we build component classifiers that are accurate and independent. There are sequential as well as parallel ensemble methods.

Explain Dimensionality Reduction in machine learning.
Dimensionality Reduction is the method of reducing the number of dimensions of any dataset by reducing the number of features. It is important because as we move into higher dimensions, the datapoints start becoming equidistant from each other which can affect the performance of unsupervised ML algorithms which use euclidean distance as the similarity function to classify datapoints. This is known as the Curse of Dimensionality. Also, it is difficult to visualize data beyond 4 dimensions.

What should you do when your model is suffering from low bias and high variance?
When the model is suffering from low bias and high variance, it is essentially overfitting, where the accuracy of train dataset is much higher than the accuracy of the test dataset. In such a situation, techniques such as Regularization can be used or the model can be simplified by reducing the number of features in the dataset.

Explain the differences between random forest and gradient boosting algorithms.
Random forest uses bagging techniques whereas GBM uses boosting techniques. Random forests mainly try to reduce variance and GBM reduces both bias and variance of a model.

# What is Naive Bayes
* Defintinly need to work on this one later

# K-fold cross validation vs Non
* K-fold cross validation is a method to evaluate a model by dividing the dataset into k equal-sized groups, or "folds."
* Meaning we training the model k times and evaulate it k times. (For Claification each iteration the model is trained, it sees the reaming k - 1 folds)
* Resulting in k evaluations
* The main goal of k-fold cross-validation is to assess the quailty of data as well as our model:
* * This accesses our model's ability to generalize to new data. Ideally we want similar evalution metrics indicating that our model is not overfitting. An indication of overfitting is a change in evaluation metrics between training and test data.
* Here is a Function:

```
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone
skfolds = StratifiedKFold(n_splits=3)
```
```
for train_index, test_index in skfolds.split(X=X_train, y=y_train_5):
    clone_clf = clone(sgd_clf)
    X_train_folds = X_train[train_index]
    y_train_folds = y_train_5[train_index]
    X_test_folds = X_train[test_index]
    y_test_folds = y_train_5[test_index]
    clone_clf.fit(X_train_folds, y_train_folds)
    y_pred = clone_clf.predict(X_test_folds)
    num_correct = sum(y_pred == y_test_folds)
    print(num_correct / len(y_pred))

##  Accuracy is generally not the preferred method to evaluate classifiers, especially if the data have imbalanced classes. Balancing multi class classification can be tricky to ensure an even disritubtion on labels.

## Best Way TO Evaluate A Classifier
* Use Confusion matrix/Cross_val_predict

```
from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(estimator=sgd_clf, X=X_train, y=y_train_5, cv=3, n_jobs=-1)
# returns a matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true=y_train_5, y_pred=y_train_pred)
```
EX: array([[26515,   776],
       [  529,  2180]])

where  [[ TP, FN],
        [FP, TN]]

### To complement a Confusion matrix we can use Recall and Precision score
* Precision = TP / (TP + FP)
* High Precision means when the model detects true then its chances of being true are high.
* Recall = TP / (TP + FN)
* High recall indicates that also when the model detects false it is more likely to be false.
* In Summary an easy way to remember is just Precision and Recall just compare truly identifed values to false positives and false negatives.
* Due to the nature of the formula is it easy to see that a lower False positve and Flase negative indicates small error in our model.
# Code

```
from sklearn.metrics import precision_score, recall_score
precision_score(y_true=y_train_5, y_pred=y_train_pred)
recall_score(y_true=y_train_5, y_pred=y_train_pred)
```

# We Also Use F1 Score, Accuracy, and Specificity
* F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
* * F1 Score basically combines Precision and Recall into a single metric.
* Accuracy = (TP + TN) / (TP + TN + FP + FN)
* * Overall "Correctness" of a  model.
* Specificity = TN / (TN + FP)
* * Measures the amount of true negatives out of all the negatives.
```
from sklearn.metrics import f1_score
f1_score(y_true=y_train_5, y_pred=y_train_pred)
#
```

We can also use this to decide our threshold value
```
y_scores = cross_val_predict(estimator=sgd_clf, X=X_train, y=y_train_5,
                             cv=3, method='decision_function', n_jobs=-1)
```

# More Interstingly we can visulize these
```
from sklearn.metrics import precision_recall_curve
precisions, recalls, thresholds = precision_recall_curve(y_true=y_train_5, probas_pred=y_scores)
def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], "b--", label='Precision')
    plt.plot(thresholds, recalls[:-1], "g-", label="Recall")
    plt.xlabel("Threshold")
    plt.grid()
plot_precision_recall_vs_threshold(precision, recall, thresholds)
plt.show()
```
"""

